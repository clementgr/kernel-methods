% This version of CVPR template is provided by Ming-Ming Cheng.
% Please leave an issue if you found a bug:
% https://github.com/MCG-NKU/CVPR_Template.

\documentclass[final]{cvpr}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[numbers]{natbib}
\usepackage{notoccite}
\usepackage{subcaption}
\captionsetup{compatibility=false}
\usepackage{graphicx}
\usepackage{bbm}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}


\def\cvprPaperID{34} % *** Enter the CVPR Paper ID here
\def\confYear{CVPR 2021}
%\setcounter{page}{4321} % For final version only


\begin{document}
	
	%%%%%%%%% TITLE
	\title{ Kernel Methods Challenge\\
		\vspace{1mm}
		\large \normalfont Predicting Whether a DNA Sequence Region is Binding Site to a Specific Transcription Factor}
	
	\author{\textbf{Cl√©ment Grisi}\\
		Team Name: clems\\
		\small \url{grisi.clement@gmail.com}
	}
	
	\maketitle
	
	\begin{abstract}
		In this report, I emphasize my work for the challenge organized as part of the Kernel Methods class. The goal of the challenge was to learn how to implement kernel-based machine learning algorithms, gain understanding about them and adapt them to structural data. To that end, teachers chose a DNA sequence classification task. My code is publicly available at \small{\url{https://github.com/clementgr/kernel-methods}}
	\end{abstract}
	
	\vspace{-3mm}
	
	\section{Introduction}
	
	Transcription factors (TFs) are regulatory proteins that bind specific sequence motifs in the genome to activate or repress transcription of target genes. Genome-wide protein-DNA binding maps can be profiled using some experimental techniques and thus all genomics can be classified into two classes for a TF of interest: bound or unbound. The challenge consists in predicting whether a DNA sequence region is binding site to a specific TF.\\
	\\
	\textbf{Dataset:} we had to work with three datasets corresponding to three different transcription factors. Each training set contained $2000$ sequences, while each testing set contained $1000$. Predictions had to be made separately for each train-test dataset pair.\\
	\\
	\textbf{Evaluation Metric:} the performance is measured using the classification accuracy.
	
	\section{Classifiers}
	
	I implemented different classifiers for the sequence classification task. Each classifier aims at solving the following optimization problem, each with a different loss function $\ell$:

	\vspace{-1.5mm}
	
	\begin{equation*}\tag{$\star$}
		\begin{aligned}
			\min_{f \in \mathcal{H}}  \hspace{1mm} \dfrac{1}{n} \sum_{i=1}^{n} \ell  \left( f(x_i), y_i \right) + \lambda \vert \vert f \vert \vert_\mathcal{H}^2
		\end{aligned}
	\end{equation*}

	\vspace{0.5mm}

	\subsection{Logistic Regression}
	
	Logistic regression is an algorithm often used in machine learning problems, which models the probability that a given input belongs to one of two different classes (binary classification). It can be viewed as solving $(\star)$ with the logistic loss $\ell  \left( f(x), y \right) = \ln \left(1+e^{-yf(x)} \right)$ and the functionnal space:
	
	\vspace{-5mm}
	
	\begin{equation*}
		\begin{aligned}
			\mathcal{H} = \{ w \in \mathbb{R}^n, \beta \in \mathbb{R}  \textit{ } \vert \textit{ } f(x) = \sigma(w^\top x + \beta) \}
		\end{aligned}
	\end{equation*}
	
	\noindent
	Equation $(\star)$ is then a smooth convex optimization problem, efficiently solved by gradient descent.
	
	\subsection{Support Vector Machine} 
	
	Support Vector Machine (SVM) is another supervised learning model, which can be viewed as solving $(\star)$ with the hinge loss:  $\ell  \left( f(x), y \right) = \max \left(1-yf(x), 0 \right)$. This time, the objective function of $(\star)$ is not smooth. Hence, it's common practice to introduce slack variables to reformulate $(\star)$ as a quadratic program, that is the minimization of a convex quadratic function with linear constraints. Quadratic programs are efficiently solved using any optimization package. I used \texttt{convexopt}.
	
	\subsection{Kernel Ridge Regression}
	
	If we consider $\mathcal{H}$ a RKHS associated to a p.d. kernel $\mathbf{K}$ on $\mathcal{X}$, then Kernel Ridge Regression (KRR) is obtained by regularizing the MSE criterion by the RKHS norm.	By the representer theorem, KRR is equivalent to:
	\vspace{-4mm}
		
	\begin{equation*}
		\underset{\boldsymbol{\alpha} \in \mathbb{R}^{n}}{\min } \textit{ } \frac{1}{n}(\mathbf{K} \boldsymbol{\alpha}-\mathbf{y})^{\top}(\mathbf{K} \boldsymbol{\alpha}-\mathbf{y})+\lambda \boldsymbol{\alpha}^{\top} \mathbf{K} \boldsymbol{\alpha}
	\end{equation*}
	\noindent
	which is a convex and differentiable problem in $\boldsymbol{\alpha}$ and admits the closed form solution:
	
	\begin{equation*}
		\begin{aligned}
			\boldsymbol{\alpha}^* = \left( \mathbf{K} + \lambda n \mathbf{I} \right)^{-1} \mathbf{y}
		\end{aligned}
	\end{equation*}
	
	\subsection{Kernel SVM} 
	
	Here again, considering $\mathcal{H}$ a RKHS associated to a p.d. kernel $\mathbf{K}$ on $\mathcal{X}$, then Kernel SVM algorithm is obtained by rewriting SVM's quadratic program using the representer theorem. This gives another QP, this time in $\boldsymbol{\alpha}$:
	\vspace{-7mm}
	\begin{center}
			\begin{equation*}\tag{\text{kSVM}}
				\begin{aligned}
					\max_{\boldsymbol{\alpha} \in \mathbb{R}^n} \quad & 2 \boldsymbol{\alpha}^{\top} \mathbf{y}-\boldsymbol{\alpha}^{\top} \mathbf{K} \boldsymbol{\alpha}\\
					\textrm{s.t.} \quad & 0  \preceq \boldsymbol{\alpha}^{\top} \mathbf{y} \preceq \dfrac{\mathbbm{1}}{2 \lambda n} 
				\end{aligned}                                      
			\end{equation*}	
	\end{center}
	\noindent
	I used \texttt{cvxpy} to solve this problem.
	
	
	\section{Working with Numeric Data}
	
	\subsection{Logistic Regression Baseline}
	
	The idea was to have a reference result against which I could compare the performances of the more sophisticated models I would later develop. I decided to go with a simple baseline: train a logistic regression on the numerical feature matrices provided by the teachers. This achieved $0.60466$ on the public leaderboard, and $0.58666$ on the private leaderboard.
	
	\subsection{Switching to Support Vector Machines}
	
	I then shifted to Support Vector Machines. Prior to fitting this model, I re-labeled inputs from $\{0, 1\}$ to $\{ -1, 1\}$. I tried both plain SVM and C-SVM and didn't observe any signficant difference. In the end, C-SVM with C=$10$ performed just as well as the logistic regression, reaching $0.60333$ on the public leaderboard, and $0.58666$ on the private leaderboard.
	
	\subsection{Using the Radial-basis Function Kernel}
	
	Enough working with raw numerical feature matrices, time to use some kernels! I moved from linear to nonlinear classifiers using the Radial-basis Function Kernel (RBF), which maps data points to Gaussian functions living in a Hilbert space $\mathcal{H}$. The RBF kernel with bandwidth $\sigma$ is given by:
	\vspace{-0.2mm}
	\begin{equation*}
		\begin{aligned}
			\mathbf{K}(\mathbf{x}, \mathbf{y})=e^{-\frac{\|\mathbf{x}-\mathbf{y}\|^{2}}{2 \sigma^{2}}}
		\end{aligned}
	\end{equation*}
	\noindent
	Once data points were mapped to $\mathcal{H}$, I solved the classification task using: 
	
	\begin{itemize}
		\item Kernel Ridge Regression: best results were achieved for $\sigma = 100$ and regularization parameter $\lambda = 10^{-7}$. This translated into $0.60000$ on the public academic, and $0.59733$ on the private leaderboard,
		\item Kernel SVM: best results were achieved for $\sigma = 100$ and $\lambda = 10^{-8}$. This translated into $0.61000$ on the public academic, and $0.59933$ on the private leaderboard, slightly outperforming previous results.
	\end{itemize} 
	\noindent
	Optimal hyperparameters $\sigma$ and $\lambda$ were found by finetuning on $30\%$ of the training data, set aside as validation set.
	
	\section{Working with Raw DNA Sequences}
	
	Once I had finished experimenting with numerical data, I started working with the raw DNA sequences. The goal was to design kernels suited for strings. I decided to go with two kernels based on substring indexation.
	
	\subsection{Spectrum Kernel}
	
	The spectrum kernel \cite{spectrum} is a sequence-similarity kernel. Given an integer $k \geq 1$, the $k$-spectrum of an input sequence is the set of all the $k$-length subsequences that it contains. If we denote by $\phi_a(k)$ the number of times the subsequence $a$ occurs in sequence $x$, and $ \mathcal{A}^k$ all possible subsequences of length $k$ from alphabet $\mathcal{A}$, the $k$-spectrum kernel is given by:
	\vspace{-0.1mm}
	\begin{equation*}
		\begin{aligned}
			\mathbf{K}_k (x, y) = \left\langle \Phi_k (x), \Phi_k (y) \right\rangle
		\end{aligned}
	\end{equation*}
	
	\noindent
	 where $\Phi_k (x) = \left( \phi_a(k)  \right)_{a \in \mathcal{A}^k}$. This kernel has the advantage of being relatively simple to implememt and fast to compute. I used Kernel SVM and fine-tuned the parameters $k$ and $\lambda$ using the $30\%$ validation set (Figure \ref{fig:acc_spectrum} and \ref{fig:lambda_spectrum}). The best results were achieved with $k=6$, giving a public score of $0.65400$ and a private score of $0.64066$.
	
	\subsection{Mismatch Kernel}
	
	The mismatch kernel \cite{mismatch} is a variant of the spectrum kernel which allows subsequences of a given length to have one -- or more -- mismatch in the compared sub-sequences. When applied to DNA, it's comes down to accounting for small mutations to persists between sequences, hence providing a biologically motivated way to compare protein sequences. Once again, I used Kernel SVM and fine-tuned the parameters $k$, $m$ (number of mismatch allowed) and $\lambda$ using the $30\%$ validation set (Figure \ref{fig:acc_mismatch} and \ref{fig:lambda_mismatch}). The best results were achieved with $k_0=8$, $k_1 = k_2 = 9$ and $m=1$, giving a public score of $0.65200$ and a private score of $0.64733$, the best score I managed to get (in average).
	
	\section{Conclusion}
	
	Through iterations in model definition and data representation, I highlight the pros and cons of the different methods I tried. In the end, I got my best score using the mismatch kernel: $0.65200$ classification accuracy on the public leaderboard and $0.64733$ on the private leaderboard. 
	
	{\small
		\bibliographystyle{unsrt}
		\bibliography{egbib}
	}
	
	\newpage
	
	\begin{figure}[h!]
	\centering
	\includegraphics[width=9cm, trim=2cm 2cm 2cm 2cm, clip]{fig/spectrum/val_acc_multiple_k_dataset0.pdf}
	\includegraphics[width=9cm, trim=2cm 2cm 2cm 2cm, clip]{fig/spectrum/val_acc_multiple_k_dataset1.pdf}
	\includegraphics[width=9cm, trim=2cm 2cm 2cm 2cm, clip]{fig/spectrum/val_acc_multiple_k_dataset2.pdf}
	\caption{\centering Validation Accuracy for Different k-Spectrum Kernels as a Function of Regularization Parameter $\lambda$}
	\label{fig:acc_spectrum}
	\end{figure}
	
	\newpage
	
	\begin{figure}[h!]
		\centering
		\includegraphics[width=9cm, trim=2cm 2cm 2cm 2cm, clip]{fig/spectrum/val_acc_lambda_multiple_k_dataset0.pdf}
		\includegraphics[width=9cm, trim=2cm 2cm 2cm 2cm, clip]{fig/spectrum/val_acc_lambda_multiple_k_dataset1.pdf}
		\includegraphics[width=9cm, trim=2cm 2cm 2cm 2cm, clip]{fig/spectrum/val_acc_lambda_multiple_k_dataset2.pdf}
		\caption{\centering Optimal Regularization Parameter $\lambda$ for Different k-Spectrum Kernels}
		\label{fig:lambda_spectrum}
	\end{figure}
	
	\begin{figure}[h!]
		\centering
		\includegraphics[width=9cm, trim=2cm 2cm 2cm 2cm, clip]{fig/mismatch/val_acc_multiple_k_dataset0_mismatch.pdf}
		\includegraphics[width=9cm, trim=2cm 2cm 2cm 2cm, clip]{fig/mismatch/val_acc_multiple_k_dataset1_mismatch.pdf}
		\includegraphics[width=9cm, trim=2cm 2cm 2cm 2cm, clip]{fig/mismatch/val_acc_multiple_k_dataset2_mismatch.pdf}
		\caption{\centering Validation Accuracy for Different Mismatch Kernels as a Function of Regularization Parameter $\lambda$}
		\label{fig:acc_mismatch}
	\end{figure}
	
	\newpage
	
	\begin{figure}[h!]
		\centering
		\includegraphics[width=9cm, trim=2cm 2cm 2cm 2cm, clip]{fig/mismatch/val_acc_lambda_multiple_k_dataset0_mismatch.pdf}
		\includegraphics[width=9cm, trim=2cm 2cm 2cm 2cm, clip]{fig/mismatch/val_acc_lambda_multiple_k_dataset1_mismatch.pdf}
		\includegraphics[width=9cm, trim=2cm 2cm 2cm 2cm, clip]{fig/mismatch/val_acc_lambda_multiple_k_dataset2_mismatch.pdf}
		\caption{\centering Optimal Regularization Parameter $\lambda$ for Different Mismatch Kernels}
		\label{fig:lambda_mismatch}
	\end{figure}
		
\end{document}