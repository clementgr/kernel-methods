% This version of CVPR template is provided by Ming-Ming Cheng.
% Please leave an issue if you found a bug:
% https://github.com/MCG-NKU/CVPR_Template.

\documentclass[final]{cvpr}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[numbers]{natbib}
\usepackage{notoccite}
\usepackage{subcaption}
\captionsetup{compatibility=false}
\usepackage{graphicx}
\usepackage{bbm}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}


\def\cvprPaperID{34} % *** Enter the CVPR Paper ID here
\def\confYear{CVPR 2021}
%\setcounter{page}{4321} % For final version only


\begin{document}
	
	%%%%%%%%% TITLE
	\title{ Kernel Methods Challenge\\
		\vspace{1mm}
		\large \normalfont Predicting Whether a DNA Sequence Region is Binding Site to a Specific Transcription Factor}
	
	\author{\textbf{Cl√©ment Grisi}\\
		Team Name: clems\\
		\small \url{grisi.clement@gmail.com}
	}
	
	\maketitle
	
	\begin{abstract}
		In this report, I emphasize my work for the challenge organized as part of the Kernel Methods class. The goal of the challenge was to learn how to implement kernel-based machine learning algorithms, gain understanding about them and adapt them to structural data. To that end, teachers chose a DNA sequence classification task. Through iterations in model definition and data representation, I highlight the pros and cons of the different methods I tried, resulting in a $0.65200$ classification accuracy on the public leaderboard and $0.64733$ on the private leaderboard. My code is publicly available at \small{\url{https://github.com/clementgr/kernel-methods}}
	\end{abstract}
	
	\vspace{-3mm}
	
	\section{Introduction}
	
	Transcription factors (TFs) are regulatory proteins that bind specific sequence motifs in the genome to activate or repress transcription of target genes. Genome-wide protein-DNA binding maps can be profiled using some experimental techniques and thus all genomics can be classified into two classes for a TF of interest: bound or unbound. The challenge consists in predicting whether a DNA sequence region is binding site to a specific TF.\\
	\\
	\textbf{Dataset:} we had to work with three datasets corresponding to three different transcription factors. Each training set contained $2000$ sequences, while each testing set contained $1000$. Predictions had to be made separately for each train-test dataset pair.\\
	\\
	\textbf{Evaluation Metric:} the performance is measured using the classification accuracy. The final score is computed by taking the average of the scores from public and private leaderboards.
	
	\section{Classifiers}
	
	I implemented different classifiers for the sequence classification task. Each classifier aims at solving the following optimization problem, each with a different loss function $\ell$:

	\vspace{-1mm}
	
	\begin{equation*}\tag{$\star$}
		\begin{aligned}
			\min_{f \in \mathcal{H}}  \hspace{1mm} \dfrac{1}{n} \sum_{i=1}^{n} \ell  \left( f(x_i), y_i \right) + \lambda \vert \vert f \vert \vert_\mathcal{H}^2
		\end{aligned}
	\end{equation*}

	\subsection{Logistic Regression}
	
	Logistic regression is an algorithm often used in machine learning problems, which models the probability that a given input belongs to one of two different classes (binary classification). It can be viewed as solving $(\star)$ with the specific logistic loss:
	
	\vspace{-2mm}
	
	\begin{equation*}
		\begin{aligned}
			\ell  \left( f(x), y \right) = \ln \left(1+e^{-yf(x)} \right)
		\end{aligned}
	\end{equation*}
	
	\noindent
	and the functionnal space:
	
	\vspace{-4mm}
	
	\begin{equation*}
		\begin{aligned}
			\mathcal{H} = \{ w \in \mathbb{R}^n, \beta \in \mathbb{R}  \textit{ } \vert \textit{ } f(x) = \sigma(w^\top x + \beta) \}
		\end{aligned}
	\end{equation*}
	
	\noindent
	Equation $(\star)$ is then a smooth convex optimization problem, efficiently solved by gradient descent.
	
	\subsection{Support Vector Machine} 
	
	Support Vector Machine (SVM) is another supervised learning model soft, which can be viewed as solving $(\star)$ with the specific hinge loss:
	
	\vspace{-2mm}
	
	\begin{equation*}
		\begin{aligned}
			\ell  \left( f(x), y \right) = \max \left(1-yf(x), 0 \right)
		\end{aligned}
	\end{equation*}
	
	\noindent
	This time, the objective function of $(\star)$ is not smooth. Hence, it's common practice to introduce slack variables to reformulate $(\star)$ as a quadratic program, that is the minimization of a convex quadratic function with linear constraints:
	
	\begin{equation*}\tag{\text{SVM}}
		\begin{aligned}
			\min_{w,z} \quad & \dfrac{1}{2} \vert\vert w \vert\vert_{2}^{2} + C \cdot \mathbbm{1}^T z \\
			\textrm{s.t.} \quad & y_i(w^Tx_i) \geq 1 - z_i \\
			& z \geq 0
		\end{aligned}
	\end{equation*}
	\noindent
	Quadratic programs are efficiently solved using any optimization package. I chose \texttt{convexopt}.
	
	\subsection{Kernel Ridge Regression}
	
	If we consider $\mathcal{H}$ a RKHS associated to a p.d. kernel $\mathbf{K}$ on $\mathcal{X}$, then Kernel Ridge Regression is obtained by regularizing the MSE criterion by the RKHS norm. 
	
	\begin{equation*}\tag{$\text{KRR}$}
		\begin{aligned}
			\min_{f \in \mathcal{H}}  \hspace{1mm} \dfrac{1}{n} \sum_{i=1}^{n}  \left( f(x_i) - y_i \right)^2 + \lambda \vert \vert f \vert \vert_\mathcal{H}^2
		\end{aligned}
	\end{equation*}
	
	\noindent
	By the representer theorem, (KRR) is equivalent to:
	\vspace{-2mm}
		
	\begin{equation*}
		\underset{\boldsymbol{\alpha} \in \mathbb{R}^{n}}{\min } \textit{ } \frac{1}{n}(\mathbf{K} \boldsymbol{\alpha}-\mathbf{y})^{\top}(\mathbf{K} \boldsymbol{\alpha}-\mathbf{y})+\lambda \boldsymbol{\alpha}^{\top} \mathbf{K} \boldsymbol{\alpha}
	\end{equation*}
	\noindent
	which is a convex and differentiable problem in $\boldsymbol{\alpha}$ and admits the closed form solution:
	
	\begin{equation*}
		\begin{aligned}
			\boldsymbol{\alpha}^* = \left( \mathbf{K} + \lambda n \mathbf{I} \right)^{-1} \mathbf{y}
		\end{aligned}
	\end{equation*}
	
	\subsection{Kernel SVM} 
	
	Here again, considering $\mathcal{H}$ a RKHS associated to a p.d. kernel $\mathbf{K}$ on $\mathcal{X}$, then Kernel SVM is obtained by rewriting (SVM) using the representer theorem. This gives a quadratic program in $\boldsymbol{\alpha}$:
	\vspace{-8mm}
	\begin{center}
			\begin{equation*}\tag{\text{kSVM}}
				\begin{aligned}
					\max_{\boldsymbol{\alpha} \in \mathbb{R}^n} \quad & 2 \boldsymbol{\alpha}^{\top} \mathbf{y}-\boldsymbol{\alpha}^{\top} \mathbf{K} \boldsymbol{\alpha}\\
					\textrm{s.t.} \quad & 0  \preceq \boldsymbol{\alpha}^{\top} \mathbf{y} \preceq \dfrac{\mathbbm{1}}{2 \lambda n} 
				\end{aligned}                                      
			\end{equation*}	
	\end{center}
	\noindent
	Again, I used \texttt{cvxpy} to solve this problem.
	
	
	\section{Working with Numeric Data}
	
	Teachers provided us with numerical feature matrices based on bag of words representation. 
	
	\subsection{Logistic Regression Baseline}
	
	The idea was to have reference results against which I could compare the performances of the more sophisticated models I would later develop. I decided to go with a simple baseline: train a logistic regression on the numerical feature matrices provided by the teachers. This achieved $0.60466$ on the public academic leaderboard.
	
	\subsection{Switching to Support Vector Machines}
		
	
	\subsection{Using the Radial-basis Function Kernel}
	
	\section{Working with Raw DNA Sequences}
	
	\subsection{k-Spectrum Kernel}
	
	\cite{spectrum}
	
	\subsection{Mismatch Kernel}
	
	\cite{mismatch}

	
	\section{Conclusion}
	
	
	{\small
		\bibliographystyle{unsrt}
		\bibliography{egbib}
	}
	
	\clearpage
	
	\begin{figure}[h!]
	\centering
	\includegraphics[width=9cm, trim=2cm 2cm 2cm 2cm, clip]{fig/spectrum/val_acc_multiple_k_dataset0.pdf}
	\includegraphics[width=9cm, trim=2cm 2cm 2cm 2cm, clip]{fig/spectrum/val_acc_multiple_k_dataset1.pdf}
	\includegraphics[width=9cm, trim=2cm 2cm 2cm 2cm, clip]{fig/spectrum/val_acc_multiple_k_dataset2.pdf}
	\caption{\centering Validation Accuracy for Different k-Spectrum Kernels as a Function of Regularization Parameter $\lambda$}
	\label{fig:acc_spectrum}
	\end{figure}
	
	\newpage
	
	\begin{figure}[h!]
		\centering
		\includegraphics[width=9cm, trim=2cm 2cm 2cm 2cm, clip]{fig/spectrum/val_acc_lambda_multiple_k_dataset0.pdf}
		\includegraphics[width=9cm, trim=2cm 2cm 2cm 2cm, clip]{fig/spectrum/val_acc_lambda_multiple_k_dataset1.pdf}
		\includegraphics[width=9cm, trim=2cm 2cm 2cm 2cm, clip]{fig/spectrum/val_acc_lambda_multiple_k_dataset2.pdf}
		\caption{\centering Optimal Regularization Parameter $\lambda$ for Different k-Spectrum Kernels}
		\label{fig:lambda_spectrum}
	\end{figure}
	
	\begin{figure}[h!]
		\centering
		\includegraphics[width=9cm, trim=2cm 2cm 2cm 2cm, clip]{fig/mismatch/val_acc_multiple_k_dataset0_mismatch.pdf}
		\includegraphics[width=9cm, trim=2cm 2cm 2cm 2cm, clip]{fig/mismatch/val_acc_multiple_k_dataset1_mismatch.pdf}
		\includegraphics[width=9cm, trim=2cm 2cm 2cm 2cm, clip]{fig/mismatch/val_acc_multiple_k_dataset2_mismatch.pdf}
		\caption{\centering Validation Accuracy for Different Mismatch Kernels as a Function of Regularization Parameter $\lambda$}
		\label{fig:acc_mismatch}
	\end{figure}
	
	\newpage
	
	\begin{figure}[h!]
		\centering
		\includegraphics[width=9cm, trim=2cm 2cm 2cm 2cm, clip]{fig/mismatch/val_acc_lambda_multiple_k_dataset0_mismatch.pdf}
		\includegraphics[width=9cm, trim=2cm 2cm 2cm 2cm, clip]{fig/mismatch/val_acc_lambda_multiple_k_dataset1_mismatch.pdf}
		\includegraphics[width=9cm, trim=2cm 2cm 2cm 2cm, clip]{fig/mismatch/val_acc_lambda_multiple_k_dataset2_mismatch.pdf}
		\caption{\centering Optimal Regularization Parameter $\lambda$ for Different Mismatch Kernels}
		\label{fig:lambda_mismatch}
	\end{figure}
		
\end{document}